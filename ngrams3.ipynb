{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnehalYu/Query-Auto-Completion-/blob/main/ngrams3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6u69c7PdH9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c055c9-5940-40f0-fd26-c02b34127c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']]\n",
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 35 items>\n",
            "35\n",
            "('f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r')\n",
            "('f', 'i', 'z', '<UNK>', '.')\n",
            "0.75\n",
            "inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import requests\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from math import exp\n",
        "\n",
        "\n",
        "# Laod NLTK, NLTK functions, and related utilities\n",
        "try:\n",
        "    # This is for using the functions from the NLTK library\n",
        "    # Testing whether library load works.\n",
        "    # Sometimes, it doesn't work on some machines because of setup issues.\n",
        "    import nltk\n",
        "\n",
        "except:\n",
        "    !pip install nltk\n",
        "\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk import FreqDist\n",
        "\n",
        "# This is to download stop words for cleaning the text\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Downloading the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        " fout.write(text)\n",
        "\n",
        "# Tolenization\n",
        "sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
        "char_tokenize = lambda x: list(x)\n",
        "toktok = ToktokTokenizer()\n",
        "word_tokenize = char_tokenize\n",
        "tokens = word_tokenize(text[:500])\n",
        "word_tokenize(sent_tokenize(text)[0])\n",
        "\n",
        "# N-gram Generation\n",
        "# Trigram\n",
        "padded_sent_tri = list(\n",
        "    pad_sequence(text[:500],\n",
        "                 pad_left=True, left_pad_symbol=\"<s>\",\n",
        "                 pad_right=True, right_pad_symbol=\"</s>\",\n",
        "                 n=3))\n",
        "tri_grams = list(ngrams(padded_sent_tri, n=3))\n",
        "#Fourgram\n",
        "padded_sent_four = list(\n",
        "    pad_sequence(text[:500],\n",
        "                 pad_left=True, left_pad_symbol=\"<s>\",\n",
        "                 pad_right=True, right_pad_symbol=\"</s>\",\n",
        "                 n=4))\n",
        "four_grams = list(ngrams(padded_sent_four, n=4))\n",
        "\n",
        "# Preprocessing (coverts the text into lowercase)\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text[:500])]\n",
        "tokenized_text[0]\n",
        "print(tokenized_text)\n",
        "\n",
        "#Model Training\n",
        "train_data, padded_sents = padded_everygram_pipeline(3, tokenized_text)\n",
        "model = MLE(3)\n",
        "#Model Fitting\n",
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)\n",
        "print(len(model.vocab))\n",
        "\n",
        "print(model.vocab.lookup(tokenized_text[0]))\n",
        "print(model.vocab.lookup('f i z x .'.split()))\n",
        "x=model.counts[['t', 'h']]['e']\n",
        "y=0\n",
        "for i in model.vocab:\n",
        "  y+=model.counts[['t','h']][i]\n",
        "print(x/y)\n",
        "\n",
        "#perplexity\n",
        "perplexity = exp(model.entropy(tokenized_text))\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import requests\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from math import exp\n",
        "\n",
        "\n",
        "# Downloading the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        " fout.write(text)\n",
        "characters = []\n",
        "count=0\n",
        "index=0\n",
        "flag=0\n",
        "\n",
        "# To count number of lines\n",
        "for char in text:\n",
        "    characters.append(char.lower())\n",
        "    if(char=='\\n'):\n",
        "      count=count+1\n",
        "    flag=flag+1\n",
        "x=0.6*flag\n",
        "\n",
        "char_unique=np.array(characters)\n",
        "cu=np.unique(char_unique)\n",
        "\n",
        "#Trainset creation\n",
        "trainsetcount=0\n",
        "trainset=[]\n",
        "trainset.append('<s>')\n",
        "trainset.append('<s>')\n",
        "for i in text[:int(x)]:\n",
        "  trainset.append(i.lower())\n",
        "  trainsetcount=trainsetcount+1\n",
        "trainset.append('</s>')\n",
        "trainset.append('</s>')\n",
        "\n",
        "#Dictionary Creation\n",
        "dict_two={}\n",
        "dict_three={}\n",
        "for i in range(trainsetcount-1):\n",
        "    pair_two = tuple(trainset[i:i+2])\n",
        "    pair_three = tuple(trainset[i:i+3])\n",
        "    if pair_two in dict_two:\n",
        "        dict_two[pair_two] += 1\n",
        "    else:\n",
        "        dict_two[pair_two] = 1\n",
        "    if pair_three in dict_three:\n",
        "        dict_three[pair_three] += 1\n",
        "    else:\n",
        "        dict_three[pair_three] = 1\n",
        "\n",
        "char_index={value: index for index, value in enumerate(cu)}\n",
        "dict_bi_to_num={value: index for index, value in enumerate(dict_two)}\n",
        "dict_num_to_bi={index: value for index, value in enumerate(dict_two)}\n",
        "\n",
        "\n",
        "#Creating an array contaioning the trigram, bigram and probabilities\n",
        "denom=0\n",
        "arr=[]\n",
        "size=[]\n",
        "size.append(len(dict_two))\n",
        "size.append(len(cu))\n",
        "table=np.zeros(size)\n",
        "for i in dict_two:\n",
        "  for c in cu:\n",
        "    arr=[]\n",
        "    arr.append(i[0])\n",
        "    arr.append(i[1])\n",
        "    arr.append(c)\n",
        "    pair=tuple(arr)\n",
        "    if pair in dict_three:\n",
        "      num = dict_three.get(pair)+1\n",
        "      denom = dict_two.get(i)+len(cu)\n",
        "      table[dict_bi_to_num.get(i)][char_index.get(c)]=num/denom\n",
        "    else:\n",
        "      num = 1\n",
        "      denom = dict_two.get(i)+len(cu)\n",
        "      table[dict_bi_to_num.get(i)][char_index.get(c)]=num/denom\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ruiQqNB-tf97"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#Testing\n",
        "testset=[]\n",
        "ans=0\n",
        "testsetcount=0\n",
        "for i in text[int(x):]:\n",
        "  testset.append(i.lower())\n",
        "  testsetcount=testsetcount+1\n",
        "for i in range(testsetcount-1):\n",
        "    test = tuple(testset[i:i+2])\n",
        "    third = testset[i+2]\n",
        "    value = table[dict_bi_to_num.get(test)][char_index.get(third)]\n",
        "    ans=ans+math.log(value)\n",
        "prob=math.exp(ans)\n",
        "print(prob)\n",
        "perp_log=(1/testsetcount)*math.log(prob)\n",
        "perp=math.exp(perp_log)'''\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "4saa_mb9G6yt",
        "outputId": "d45819b7-b054-4435-bd0c-6c056c39aa23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-37ddb08fc71c>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mperp_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtestsetcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mperp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperp_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: math domain error"
          ]
        }
      ]
    }
  ]
}